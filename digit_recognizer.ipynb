{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core and utility packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# keras\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import models\n",
    "\n",
    "import os\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/Nickfis/Documents/Projects/digit_recognizer'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"/home/Nickfis/Documents/Projects/digit_recognizer/data/train.csv\")\n",
    "test = pd.read_csv(\"/home/Nickfis/Documents/Projects/digit_recognizer/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking structure of data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking structure of data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "g = sns.countplot(Y_train)\n",
    "\n",
    "# counting the occurrences of the labels. No imbalance problem. \n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free some space\n",
    "del train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Null and missing values\n",
    "\n",
    "This would tell us about possible corrupted images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "X_train.isnull().any().sum()\n",
    "# no missing data in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().sum()\n",
    "# no missing data in the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "In context of computer vision, with normalization of data is meant here, to grayscale all the images. This is done on the one hand to reduce the effect of illumination's differences and has a big computational advantage, since now our feature space is just between 0 and 1, instead of ranging from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape\n",
    "\n",
    "The dataset from Kaggle gives us a 784 feature vector for each of the images. This corresponds to the 28 pixel height and width of all the images. The following code will turn this array into a 3-dimensional array. Besides the height and width of an image we have to take care of its color (usually). Here the images are gray scaled instead of RGB. In case of a colored picture we would have 784x3 = 2352 features per image. \n",
    "In this case we will therefore have 42000 28x28x1 matrices in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding\n",
    "One-hot encoding of target label. Instead of one number, which can take on values between 0 and 9, we will have a one-hot-vector for each observation with 10 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Split training and validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some examples\n",
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mdelling the convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 26, 26, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 10, 10, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 814,970\n",
      "Trainable params: 814,778\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> [Dense -> Dropout]*2 -> Softmax\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = (28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "\n",
    "# Turn epochs to 30 to get 0.9967 accuracy\n",
    "epochs = 30 \n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-fad8e4f35954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                               , callbacks=[annealer])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                      str(validation_data))\n\u001b[1;32m    139\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 140\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[annealer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
